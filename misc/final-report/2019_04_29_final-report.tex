\PassOptionsToPackage{utf8}{inputenc}
\documentclass{bioinfo}

\usepackage{url}
\copyrightyear{2019} \pubyear{2019}

\access{Due: 04.05.2019}
\appnotes{}

\begin{document}
\firstpage{1}

\subtitle{JavaScript Technology 2019}

\title[JavaScript on the server. A Node.js crash course]{JavaScript on the server. A Node.js crash course}
\author{Julian Nalenz, Valentin Dimov and Robert Dillitz}

\abstract{\textbf{Motivation:} Node.js is increasingly important in today's web and other software development and internet infrastructure in general. Allowing developers to write source code in JavaScript for client and server alike and using the language's advantages regarding performance and modularity, as well as extending it with an efficient package manager and a platform-independent API shaped Node.js to be an integral part of modern development.\\
\textbf{Results:} This paper explains the framework's basic features, several widely used libraries, and concludes with a practical project that dealt with predicting the likelihood of death of characters from the popular book and TV series "Game of Thrones".\\
\textbf{Contact:} \href{nalenz@in.tum.de}{nalenz@in.tum.de}}

\maketitle

\section{Introduction}

Node.js is an asynchronous, event-driven JavaScript runtime that uses a non-blocking I/O model to support lots of concurrent connections at once. It was built to create dynamic, highly scalable network applications in JavaScript.\\ Prior to the start of the Node.js project in 2009, JavaScript was not the general-purpose language it is today, but rather a niche scripting language supported by internet browsers to execute code on the client side. Thanks to the creation of the ECMAScript 2015 standard which opposed distinct design flaws the language had accumulated over the years, JavaScript in combination with Node.js became one of the most important coding environments, not only as back-end solution for web services (which is the main focus here), but also for command-line tools and even desktop applications like Slack, Discord and Microsoft's Visual Studio Code, all using the Electron (https://electronjs.org/) framework.\\
Node.js is built on Google's V8 JavaScript engine, written in C++ and originally developed for the company's Chrome browser, which uses just-in-time compiling to directly translate JavaScript into fast running machine code. The V8 engine rests on the sixth version of the ECMAScript standard also known as \texttt{ES6}.

\subsection{Installing \& Using Node.js}
\subsubsection{Installation}
When downloading Node.js (https://nodejs.org/) one can choose between two versions: \texttt{Long-Term Support (LTS)} and \texttt{Current}. Which version to build a project on depends on the requirements of absolute stability at the cost of rare updates and effort to handle more frequent updates that the latest performance and feature improvements entail.
Both versions are free \& open source and maintained by the Node.js Foundation, an industry consortium, operating under an open governance model. After using the downloaded installer one can run \texttt{node -v \&\& npm -v} in the terminal to make sure the installation has succeeded.

\subsubsection{REPL}
Node.js comes with a "Read-Evaluate-Print-Loop" (REPL), which might be familiar from languages like Python or OCAML, that lets one run JavaScript code directly in the terminal and see its result. This is useful for quickly trying different statements, experimenting and debugging. The REPL is started by just typing \texttt{node} into the terminal prompt and closed by pressing \texttt{CTRL+D}.

\subsubsection{Running Programs}
Running programs in Node.js is as simple as it gets. After making sure one is in the working directory of the program, it is started by executing \texttt{node <name\_of\_the\_program>} in the terminal.

\subsubsection{Naming Conventions}
Although it is not required, filenames end generally with the filename extension \texttt{.js} and the main file of the program is called either \texttt{index.js}, \texttt{app.js} or \texttt{server.js}.

\section{Single-Threaded Event Loop Model}

A main selling point of Node.js is its single-threaded event loop model. In comparison to other traditional server implementations like the standard C or Java solution, where everytime a new connection is established a new thread or even process is created to serve the incoming request, Node.js makes use of a different approach. Here only one single thread, called the event loop, handles every upcoming demand. To make this model work, the code executed by Node.js has to be written in an asynchronous, non-blocking manner, which is outlined next.

\subsection{Asynchronous Programming}
As always in computer science when analyzing efficiency and scalability, I/O-operations such as database accesses, file reads / writes and network operations, all of which one of course needs to make plenty of use of in a server application, make or break the deal. Here is the part where the asynchronous, non-blocking programming comes in play. Instead of executing the time consuming I/O-operation directly in the event loop, which of course would make every other request have to wait for the I/O-call to finish and therefor slow down the whole server unbearably, Node.js "outsources" the costly operation to a pool of POSIX worker threads provided by a libary called libuv (https://libuv.org/). The POSIX worker thread then executes the system call in the background to keep the event loop free and running.

\subsubsection{Callbacks}
First it is important to understand the concept of a callback which is the primary way asynchronicity is handled in Node.js. A callback is a function that is passed as an argument (in our case to Node's built-in module methods) and called upon completion with the result of the initial method call. All Node.js API calls support callbacks in order to produce non-blocking code. The callback method is always the last argument of the API call, most of the time created directly within the argument brackets using the ES6 arrow notation: 
\begin{verbatim*}
(argument0, argument1) => {function body}
\end{verbatim*}
In general the first argument of the callback function is used for error handling (argument0 in the notation clarification above).

\subsubsection{Modules}

Modules bundle functionality into easily importable packages. 
//TODO
\begin{verbatim*}
    var crypto = require('crypto');
\end{verbatim*}
\subsubsection{Code Example}
Here are two code examples that show the different approach one has to get used to when programming in an asynchronous, non-blocking fashion. In both cases we want to open the file \texttt{file.md}, read its content, print it onto the console and delete it afterwards.
\begin{verbatim*}
    //synchronous & blocking example
    const fs = require('fs');
    const data=fs.readFileSync('/file.md');
    console.log(data);
    fs.unlinkSync('/file.md');

    //asynchronous & non-blocking example
    const fs = require('fs');
    fs.readFile('/file.md', (readFileErr, data) => 
    {
        if (readFileErr) throw readFileErr;
        console.log(data);
        fs.unlink('/file.md', (unlinkErr) => 
        {
            if (unlinkErr) throw unlinkErr;
        });
    });
\end{verbatim*}

The first example gets the task done in the classic synchronous 
// TODO

\section{NPM}

\subsection{Basics}
// TODO

\subsection{Express}

\subsubsection{Introduction}

Node.js allows programmers to develop the entire stack of a web application in JavaScript only. In its most basic form, such an application consists of a frontend and a backend. The frontend uses well-known technologies like HTML, CSS and client-side JavaScript to display content to the end user; nothing specific about that will be discussed here. On the other hand, the backend serves this content, i.e. it may be a simple HTTP server that answers clients' requests to fetch various URLs.

The Hypertext Transfer Protocol, commonly known as HTTP, is used for virtually any web-based application. Although it appears simple, its details are sometimes unintuitive and incorrect implementations may pose severe security risks to the server itself and thus all other clients as well. This is the motivation for a library like Express: developers need a simple way to, among many other more complex tasks, establish routes and serve static content. Additionally, as that framework is remarkably flexible, more sophisticated applications can build on it to better suit their needs.

Having this kind of HTTP server framework means that application prototyping becomes easier, faster and less error-prone, which is a good fit for today's agile software development techniques. Moreover, developers can focus on the application logic itself, as for simple projects, no complex server setup is necessary. Express describes itself as a "fast, unopinionated minimalist web framework for Node.js" \cite{expresshome}.

\subsubsection{Installation and initialization}

After having initialized a Node.js project like described before in the NPM basics, the developer needs to install the \texttt{express} module by entering the following command on the respective shell \cite{expressinstalling}:

\begin{verbatim*}
    npm install --save express
\end{verbatim*}

Next, note that applications using Express just need a single file to operate: the project's main \texttt{index.js}. This file has to contain the following JavaScript code to first import the Express library and subsequently create a server; the latter is assigned to a variable commonly named \texttt{app}, as it will contain the entire server-side state of the eventual web application. Having this pattern also means that creating multiple servers in the same application is possible; of course all servers will need to listen on different ports in this case.

\begin{verbatim*}
    const express = require('express');
    const app = express();
\end{verbatim*}

\subsubsection{Basic routes and launching}

In general, HTTP is a protocol that expects clients to specify the location of the information they would like to retrieve in form of a path, also known as \textit{route}. When a user enters an URL like \texttt{http://example.com/} into their browser, the first part specifies which protocol to use (\texttt{http://}), then comes the domain of the server itself (\texttt{example.com}), and finally the path to access (\texttt{/}). In this example, a single forward slash as the path refers to the index page, commonly refered to as \textit{homepage}, of the respective domain. In Express, a handler to serve content for this route can be installed like this \cite{expresshelloworld}:

\begin{verbatim*}
    app.get('/', (req, res) =>
        res.send('Hello World!'));
\end{verbatim*}

Note the use of an asynchronous callback in ES6 syntax here: when any client tries to access the \texttt{/} path, this function is run. Differing slightly from the previously encountered form of callbacks, which simply compute a result asynchronously, this one is adjusted to fit the needs of HTTP. Thus, it gets two parameters, a \textit{request} and a \textit{response} object. The former contains all information the user included in their request, e.g. query parameters, HTTP headers (including cookies), the client's IP address etc. On the other hand, the latter contains functions to send data back to the client in order to generate a response.

This piece of code is a good example to show all the abstractions Express provides the developer: he does not need to care about HTTP status codes, parsing and sending HTTP headers, or even byte streams or system-level sockets anymore. To finally start the server, the following code might be used:

\begin{verbatim*}
    app.listen(8080);
\end{verbatim*}

When this function is called, Express expects the \texttt{app} instance to be fully configured with all desired routes, as it will block indefinitely. The port given to it is generally arbitrary, but needs to be in the valid port range and unoccupied by any other application of course. Running the server finally only consists of executing \texttt{node index.js} on the respective shell, then one can enter the URL \texttt{http://localhost:8080/} in any browser to see the result.

\subsubsection{More features}

In addition to GET, Express also supports all other HTTP methods commonly used to implement REST services, i.e. POST, PUT and DELETE. To use them, the only necessary change to the route specification above is to use \texttt{app.post}, \texttt{app.put} or \texttt{app.delete} instead.

Sometimes, developers also need an even simpler form of serving content and do not require dynamically generated responses: static assets. This means that the server shall publish all files from a designated directory unchanged, including all files in subdirectories. That behavior can be achieved by calling:

\begin{verbatim*}
    app.use(express.static(dirName));
\end{verbatim*}

Routes may have more complex definitions as well, as simple patterns can be used \cite{expressrouting}. For example, the \texttt{/xy?z} route will match client requests to \texttt{/xz} and \texttt{/xyz}; the \texttt{y} character becomes optional. The \texttt{+} wildcard matches paths having the previous character repeated one or more times, the \texttt{*} wildcard matches any number (including no) characters. In addition to that, any JavaScript regular expressions are allowed as well. Moreover, routes like \texttt{/article/:articleId} can include parameters: when a client requests the path \texttt{/article/42}, the callback receives \texttt{42} in \texttt{req.params.articleId}.

The Express library itself is very lightweight and only provides basic functionality to serve content via HTTP. Yet, it can be extended with plugins. Three examples for that are \texttt{body-parser}, which is able to parse URL-encoded form data, \texttt{cookie-parser}, which can extract information about all set cookies from HTTP's \texttt{Cookie} header, and \texttt{compression}, which automatically compresses data sent back to the client using gzip to improve performance.

\subsubsection{Limitations}

More advanced functionality, especially regarding dynamic websites, cannot be achieved with Express directly. Examples include automatic HTML generators from templates or bundling client-side JavaScript code in a single file to improve maintainability and efficiency.

Still, this is not what Express is supposed to be anyway: a lightweight HTTP server library. Regarding that, Express does not lack any features necessary in modern, real life development environments.

\subsection{Other libraries}
\begin{itemize}
    \item One
    \item Two
    \item Three
\end{itemize}

\section{Our project}

\subsection{The Neural Network approach}

\subsubsection{Introduction}

Game of Thrones is not only a popular book series, but also an internationally successful TV show, having millions of enthusiastic viewers each episode. In its fantasy-themed world, well-known and sometimes beloved characters frequently die; these highly important events almost always come as a surprise.

Our goal was to predict when these deaths will happen. We found that information about a character's background, for example the house he is a part of, his allegiances and so on, were not only readily available in a community-driven wiki, but also sufficiently complete and numerous. Together with modern machine learning approaches, this allowed us to rephrase this task as a binary classification problem, which a neural network could then be trained for.

In general, this process first consisted of formatting the data in two steps, in which dead characters for training and alive characters for the final predictions were separated. Afterwards, it was possible to train a neural network with the Python-based machine learning framework Keras, which is based on Google's TensorFlow \cite{keras}, and eventually predict the percentage likelihood of survival/death for alive characters 20 years into the future.

\subsubsection{Input data transformations}

We had two different datasets containing information about characters of Game of Thrones: one for the book series and one for the TV show. Although the show is based on the book, the two have been starting to diverge more and more, leading to us treating them as independent and virtually unrelated datasets. The following explanations will elaborate mostly on the show data; as the process is directly applicable to the book data as well, we will only mention which kinds of information were used in this context shortly in the end.

The database provided us with a large JSON array, containing an entry in the form of a JSON object for every single character who has ever appeared in the show and who has a wiki article. Relevant properties from these object for the predictions are: age, allegiances, appearances, birth, father, gender, house, lovers, mother, name, page rank, religion, siblings, spouse and titles. These properties can then be aggregated and grouped into whether they describe a scalar value (e.g. the page rank of the character's wiki page), or a one-/multiple-hot vector.

The simplest scalar values were binary, i.e. the "isMale" and "isBastard" properties were set to 1.0 if the character has this property and 0.0 otherwise.

For the other scalar values, the first input data transformation step found the maximum value and then divided every value by this maximum. For example, Jon Snow had the highest unnormalized page rank, 1749, which was then normalized to 1.0. Consequently, Arya Stark with a page rank of 1014 was assigned the relative scalar value $1014/1749=0.5798$ for this property. Similarly, the number of a character's relatives (total number of known family members, i.e. father, lovers, mother, siblings and spouse) and the number of battles he commanded was calculated.

Other properties were not transformable into a single value. Because all of the ones we looked at had a pre-defined set of possible values, the first input data transformation step also aggregated the set of these values and converted the arrays containing these values associated to a character to a list of indices instead. As an example, 130 episodes of the TV show were captured, which are stored in a separate, alphabetically sorted array. Among others, Daenerys Targaryen appeared in the episodes Winter is Coming, The Kingsroad, Lord Show etc., which were then converted into an array containing the indices 72, 47 and 26.

The same approach was then taken for the possible allegiances, and the titles a character has, of which there were 130 and 118, respectively.

Next, this data had to be transformed to input vectors for a neural network. In this context, an input vector is simply a list of multiple single-precision floating point values, all of which are between 0.0 and 1.0. Additionally, as the dead characters serve as training data, the corresponding labels, i.e. the values the neural network is supposed to output, need to be stored as well.

\subsubsection{Neural network input vectors}

Basically, an input vector consists of the scalar values and one-hot vectors for the properties with a pre-defined set of possible values, all just appended to each other, resulting in a vector with several hundred dimensions.

Yet, the neural network is supposed to predict the likelihood of death/survival for consecutive years. That's why we introduced the concept of expanding the input data, which would normally just consists of one vector for each character, along the "age" property. This not only results in several thousand input vectors available for use during training, validation and testing, but also represents an obvious property of humans: old age means higher probability of death. To understand this more thoroughly, consider the following example.

Assume there are $n+1$ possible ages (i.e. the minimum age is $0$ and the maximum age is $n$). Also assume that a character died at age $m$, where $0\leq m\leq n$. Next, one loops through each possible age for each character; say the currently looked at age is $a$, where $0\leq a\leq n$. A one-hot vector with $n+1$ dimensions representing this age will be appended to the current input vector with the character's other properties already serialized, then a separate label (i.e. likelihood of survival the network is supposed to output) needs to be generated. If $a<m$, the value 1.0 is assigned to this label, and 0.0 otherwise. Eventually this means that the neural network will only have one single output dimension.

Note that this process will create $c*(n+1)$ vectors for training in total, where $c$ is the number of dead characters suitable for being used as training data. Effectively, as the maximum age for the show, ignoring outliers, was $n=85$, together with $c=82$, it was possible to generate 7052 training vectors.

This process also makes it very easy for the network to predict the likelihood of survival into the future for alive characters: take the other serialized properties as they are, then add the age the character has in the year one would like to predict his likelihood of survival for as a one-hot vector.

\subsubsection{Training results}

All of the previously explained data formatting steps were implemented in pure NodeJS-based JavaScript. For the machine learning part itself, on the other hand, Python with the Keras library was used, which just expects training data and labels as NumPy vectors and outputs statistics about the neural network during training in real time. As the input vectors and output labels were just written to a binary file by the respective formatter, the data could be loaded without much effort on the Python side.

The show's neural network had a sequential architecture, with four ReLU-activated hidden layers with a 0.7 dropout having 1000, 500, 250 and 100 dimensions, respectively. The output layer with just a single dimension was sigmoid-activated; the input layer had 413 dimensions. Keras was instructed to train using the RMSprop optimizer for eight epochs and with a batch size of 32. 10\% of the training data was used for validation. After training, a 81.00\% accuracy on the training data and 84.56\% accuracy on the validation data was reached.

Note that these initial predictions, which were also published later, did not have backing of a testing set. This issue was approached later by using 63.75\% of datapoints generated from dead characters for training, 11.25\% for validation and 25\% for testing. The gender distribution between training+validation and testing datasets was the same. Upon retraining in twelve epochs, this resulted in 90.37\% training accuracy, 72.31\% validation accuracy and 84.69\% testing accuracy. The predictions created by this new neural network, which also contained some changes regarding its number of dimensions, which rose to 428, were never made publicly available though.

For the book's neural network, a similar approach was used: three ReLU-activated hidden layers with a 0.8 dropout having 500, 250 and 100 dimensions, respectively. This network had 1561 input dimensions, as the following character properties were used: male, page rank, number of relatives, age, culture, house, house region, allegiances, books, locations and titles. The maximum age was 99, so 188 dead characters usable for training were turned into 18800 input datapoints, 20\% of which were used for validation; no attempts to use an explicit testing set was made. Training eventually resulted in 88.75\% training accuracy and 89.92\% validation accuracy.

\subsubsection{Summary}

The approach of expanding the data along the "age" property turned out to be a good choice for this kind of project. Game of Thrones is a work of fiction, which consists of invented characters, so each one, especially if they have their own wiki article, is special in its own way and is not necessarily generalizable on other characters. When combining this with the immense diversity of properties of properties a character has and the wide lack of complete information, recollecting basic human features is necessary: old age means that a character has experienced more in general and in a fantasy world like the one we were dealing with, this means an increased likelihood of death. This trait was modeled rather well by the used approach.

As specified before, it was very easy to predict like likehihoods of survival for all alive characters 20 years into the future with this model. Most of these predictions showed that the likelihood of survival decreased over time like expected, although significant outliers often appeared.

We concluded that finally, because of the increased level of incertainty, most of the generated predictions were rather suitable for being released to the public, as they rarely settled at almost exactly 0\% or 100\% PLOD. Similar to the Bayesian Survival Analysis approach explained next, all implementation details are available in the project's repository at \url{https://github.com/nalenz/got.predictions}.


\subsection{The Bayesian Survival Analysis Approach}
\subsubsection{The Model}
Another model we considered for our predictions is based on Bayesian survival analysis, which aims to apply Bayesian inference to predicting the risk of an event occurring (or not occurring) up to a certain time. When applied to death (and, by extension, Game of Thrones), this approach allows us to calculate the \textbf{hazard rate} (the instantaneous probability of death, or, with discretization, the probability of death in a given year) for each character depending on their attributes. This, in turn, allows us to construct a \textbf{survival function} for each character, which describes the probability of death \textbf{not} occurring until a certain point in their life. The survival function is effectively our longevity function. The model was trained using Python's \textbf{pymc3} library. \cite{pymc3} 

For the hazard rate, we used Cox's proportional hazards model, which assumes that there is a common \textbf{base hazard} for all characters, and the presence of any attribute increases or decreases that hazard proportionally (e.g. being a male makes you $6o\%$ more likely to die. For continuous attributes, the influence is then assumed to be exponential. Our model is mostly based on an article on PyMC3's website \cite{bayesian-surv}, only adapted to accept multiple attributes.

One thing that should be noted is that for the book, our model considers each character's timeline starting at their birth, whereas in the show, mostly due to lacking birth dates, but also because the timespan covered is much shorter, the characters' timelines start at the beginning of the show in the in-universe year 298. For book data, base hazard is independent for all years of a character's life, whereas in the show, the base hazard is assumed to be the same for every year (otherwise, we'd only be able to infer base hazard for the years the show has already taken place, but not the years we needed to predict).

\subsubsection{Usage}
Training this model involves several steps with different components in our package (TODO link to github page).

\begin{enumerate}
    \item \texttt{/data/book/refetch.sh} : Refetch the mined data for the book.
    \item \texttt{node workers/formatter-bayesean-book} : Extract the attributes from the mined data and output them in a JSON file (training{\textunderscore}book{\textunderscore}characters.json).
    \item \texttt{node workers/predictors-bayesian\\/predictor-bayesean-book} : Run the predictor, which will train the model, use it to predict the longevity of both alive and dead characters, then output its results in book{\textunderscore}predictor{\textunderscore}output.json
    \item \texttt{node workers/postprocessor-bayesean-book} : Extract longevity functions for alive characters and format the data in a format suitable for upload in book{\textunderscore}predictions.json
    \item \texttt{node workers/uploader-predictions-bayesean} : Update the predictions that have changed using the website's API.
    \item \texttt{node workers/uploader-attributes-bayesean} : Update the attributes and their influences on the website's API.
\end{enumerate}

Keep in mind that before uploading, the correct security token needs to be configurated for the API calls in the code. To run the predictor on the show data, one only needs to instead use \texttt{/data/show/refetch.sh}, \texttt{formatter-bayesean-show}, \texttt{predictor-bayesean-\\show} and  \texttt{postprocessor-bayesean-show}. The uploaders update predictions and attributes for both models.

\subsubsection{Results}
The attributes we considered for the characters in the last iteration of this model are:\\
For the book:
\begin{enumerate}
    \item \textbf{House allegiance}: Frey, Greyjoy, Lannister, Martell, Stark, Targaryen
    \item Being a Dornishman
    \item Being a Northman
    \item Having children
    \item Having titles
    \item Being someone's heir
    \item Being married
    \item Being male
\end{enumerate}
For the show:
\begin{enumerate}
    \item \textbf{House allegiance}: Arryn, Baratheon of Dragonstone, Baratheon of King{'}s Landing, Bolton, Frey, Greyjoy, Lannister, Martell, Stark, Targaryen, Tarly, Tully, Tyrell
    \item Having lovers
    \item Having titles
    \item Being a major character
    \item Being married
    \item Being male
\end{enumerate}

Some of the most influential attributes found were, for the books: House Targaryen (risk of death increased by $135\%$), House Stark (risk increased by $121\%$) and House Greyjoy (risk decreased by $43,1\%$). For the show, those were being male (risk  increased by $102\%$), being married (risk decreased by $55,7\%$) and, interestingly, House Tarly (risk decreased by $62,8\%$).

Something that can be observed in almost all cases is that, perhaps due to the large amount of censored inputs, the model often overestimates the survival chances of dead characters and assumes most living characters have little chance of immediately dying. A possible source of bias is that not all characters are useable for the model - for example, if critical data is missing.\\
Some statistics about the model's last iteration:
\begin{enumerate}
    \item For the \textbf{book}, the average probability of death at the moment of death for dead characters was approximately $36.3\%$
    \item For the \textbf{book}, the average probability of death right after the current year for alive characters was approximately $22.2\%$
    \item For the \textbf{show}, the average probability of death at the moment of death for dead characters was approximately $37.8\%$
    \item For the \textbf{show}, the average probability of death right after the current year for alive characters was approximately $47.6\%$
\end{enumerate}
It is possible that the large amount of censored inputs caused the model to underestimate the base hazard, which, despite the model catching onto some patterns, still massively influences the result.

We concluded that this model can provide some statistical insight into the influence of various properties on death hazard, but it is not particularly useful for predicting the often plot-driven deaths that the show is known for. That is why, for our predictions, we settled on the neural network.

%\enlargethispage{12pt}



%\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
%
%\bibliographystyle{plain}
%
%\bibliography{Document}


\begin{thebibliography}{}

\bibitem{bayesian-surv}  Survival analysis article: https://docs.pymc.io/notebooks/survival{\textunderscore}analysis.html (retrieved April 27, 2019)

\bibitem{pymc3} PyMC3 API: https://docs.pymc.io/api.html (retrieved April 27, 2019)

\bibitem{keras} Keras: https://keras.io/ (retrieved April 29, 2019)

\bibitem{expresshome} Express: https://expressjs.com/ (retrieved April 29, 2019)
\bibitem{expressinstalling} Installing Express: https://expressjs.com/en/starter/installing.html (retrieved April 29, 2019)
\bibitem{expresshelloworld} Express "Hello World" example: https://expressjs.com/en/starter/hello-world.html (retrieved April 29, 2019)
\bibitem{expressrouting} Express routing: http://expressjs.com/en/guide/routing.html (retrieved April 29, 2019)

\end{thebibliography}



\end{document}
